{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional, Any\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "with open('names.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "vocab = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i for i,s in enumerate(vocab, start=1)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test split\n",
    "random.shuffle(words)\n",
    "\n",
    "train_split = .8; val_split = .1\n",
    "\n",
    "n1 = int(train_split * len(words))\n",
    "n2 = int((train_split + val_split) * len(words))\n",
    "\n",
    "X_train, Y_train = build_dataset(words[:n1])\n",
    "X_val, Y_val = build_dataset(words[n1:n2])\n",
    "X_test, Y_test = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility fn to compare gradients calc manually v/s via PyTorch\n",
    "def cmp(s, dt: torch.Tensor, t: torch.Tensor):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f\"{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "#IMP FOR THIS NOTEBOOK: CHANGE SOME INIT VALUES AS ZEROS MAY MASK SOME INCORRECT GRADIENT IMPL, **JUST FOR LEARNING PURPOSES**\n",
    "\n",
    "# build MLP\n",
    "\n",
    "# MLP structure params\n",
    "emb_dim_size = 10\n",
    "hidden_layer_size = 64\n",
    "\n",
    "# MLP params\n",
    "C = torch.randn((vocab_size, emb_dim_size), generator=g).float() # represent each character in dim space\n",
    "#Layer 1\n",
    "W1 = torch.randn((block_size * emb_dim_size, hidden_layer_size), generator=g).float() # weights for hidden layer, for each neuron, it will receive block_size number of i/p, each i/p of dimension dim_size\n",
    "W1 = W1 * (5/3) / ((block_size * emb_dim_size) ** 0.5) # kiming_init - to \"fight the contraction of tanh\", preserve gaussian std. -> to resolve tanh saturation\n",
    "b1 = torch.randn(hidden_layer_size, generator=g).float() # each neuron will have 1-D bias # USING BIAS JUST FOR FUN\n",
    "b1 = b1 * 0.1 # normally: 0.001 # to resolve tanh saturation\n",
    "# Layer 2\n",
    "W2 = torch.randn((hidden_layer_size, vocab_size), generator=g).float() # weights for output layer, each neuron of hidden layer fully connected to each neuron of output layer\n",
    "W2 = W2 * 0.1 # to normalize loss at initialization\n",
    "b2 = torch.randn(vocab_size, generator=g).float() # num_neurons in output layer equal to vocab size, to represent probs for each character\n",
    "b2 = b2 * 0.1 # normally 0 # to normalize loss at initialization\n",
    "\n",
    "## batch norm params: want the distribution to be more flexible, not constrict it to be always Gaussian, just need at initialization.\n",
    "# batch_norm_gain = torch.ones((1, hidden_layer_size))\n",
    "# batch_norm_bias = torch.zeros((1, hidden_layer_size))\n",
    "# change from normal, normal above\n",
    "batch_norm_gain = torch.randn((1, hidden_layer_size)) * 0.1 + 1.0\n",
    "batch_norm_bias = torch.randn((1, hidden_layer_size)) * 0.1\n",
    "\n",
    "params = [C, W1, b1, W2, b2, batch_norm_gain, batch_norm_bias]\n",
    "\n",
    "running_mean = torch.zeros((1, hidden_layer_size))\n",
    "running_std = torch.ones((1, hidden_layer_size))\n",
    "\n",
    "print(sum(p.nelement() for p in params))\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "n = batch_size\n",
    "# construct minibatch\n",
    "ix = torch.randint(0, X_train.shape[0], (n, ), generator=g)\n",
    "Xb, Yb = X_train[ix], Y_train[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4235, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FORWARD PASS: CHUNKED INTO SIMPLE, MANAGEABLE STEPS THAT CAN BE BACKPROP'ED ONE AT A TIME\n",
    "\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "# -----------Linear layer 1---------------\n",
    "hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "# ----------BatchNorm layer---------------\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = (\n",
    "    1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    ")  # Bessel's correction, dividing by (n-1) not n, for mean calc\n",
    "with torch.no_grad():\n",
    "    epsilon = 1e-5\n",
    "bnvar_inv = (bnvar + epsilon) ** -0.5  # inverse of \"bnstd\"\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = batch_norm_gain * bnraw + batch_norm_bias\n",
    "# ----------Non-linearity---------------\n",
    "h = torch.tanh(hpreact)\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "# cross-entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability ??\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# Pytorch backward pass\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "\n",
    "\n",
    "gradvars = [\n",
    "    logprobs,\n",
    "    probs,\n",
    "    counts,\n",
    "    counts_sum,\n",
    "    counts_sum_inv,\n",
    "    norm_logits,\n",
    "    logit_maxes,\n",
    "    logits,\n",
    "    h,\n",
    "    hpreact,\n",
    "    bnraw,\n",
    "    bnvar_inv,\n",
    "    bnvar,\n",
    "    bndiff2,\n",
    "    bndiff,\n",
    "    hprebn,\n",
    "    bnmeani,\n",
    "    embcat,\n",
    "    emb\n",
    "]\n",
    "\n",
    "for t in gradvars:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 10]), torch.Size([27, 10]), torch.Size([32, 3]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape, C.shape, Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhpreact        | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dbatch_norm_gain | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "dbatch_norm_bias | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dbnraw          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dbnvar_inv      | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dbnvar          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dbndiff2        | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "dbndiff         | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "dbnmeani        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dhprebn         | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "dembcat         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dW1             | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
      "db1             | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "demb            | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dC              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n"
     ]
    }
   ],
   "source": [
    "#backprop through the grads array manually\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n # loss = -dlogprobs[range(n), Yb].mean() => loss = -(a + b + c) / 3 => dloss/da = -1/3 n = 32 in our case (batch size)\n",
    "\n",
    "dprobs = 1.0/probs # logprobs = probs.log() => log(a) => dlogprobs/da = 1/a\n",
    "dprobs = dprobs * dlogprobs # chain rule\n",
    "\n",
    "# c = a * b\n",
    "# tensor\n",
    "# c[3x3] = a[3x3] * b[3x1]\n",
    "# c1 = a11*b1  a12*b1  a13*b1\n",
    "# c2 = a21*b2  a22*b2  a23*b2\n",
    "# c3 = a31*b1  a32*b2  a33*b3\n",
    "\n",
    "dcounts_sum_inv = counts # probs = counts * counts_sum_inv => probs = b * a, dprobs/da = b\n",
    "dcounts_sum_inv = dcounts_sum_inv * dprobs # chain rule\n",
    "dcounts_sum_inv = dcounts_sum_inv.sum(1, keepdim=True) # sum through pytorch replication => m1 [3X1] => m2 [3x3] so gradients would be summed across rows\n",
    "# dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "\n",
    "# dcounts --- 1st contribution (from probs)\n",
    "dcounts = counts_sum_inv # probs = counts * counts_sum_inv => probs = b * a, dprobs/db = a\n",
    "dcounts = dcounts * dprobs # chain rule #TODO: Investigate why dcounts *= dprobs doesn't work here. Look at shapes for hint.\n",
    "\n",
    "dcounts_sum = (-counts_sum ** -2)# counts_sum_inv = counts_sum ** -1, y = x**-1, dy/dx = -x**-2\n",
    "dcounts_sum = dcounts_sum * dcounts_sum_inv # chain rule\n",
    "\n",
    "# dcounts --- 2nd contribution (from counts_sum)\n",
    "# a[3x3] -> b[3x1] (rows summed)\n",
    "# a11 a12 a13   b1 = a11 + a12 + a13\n",
    "# a21 a22 a23   b2 = a21 + a22 + a23\n",
    "# a31 a32 a33   b3 = a31 + a32 + a33\n",
    "\n",
    "# db/da = [1 1 1] (3x1)\n",
    "\n",
    "dcounts = dcounts + (torch.ones(counts.shape) * dcounts_sum)\n",
    "\n",
    "# dnorm_logits\n",
    "# counts = norm_logits.exp() # both of same shape\n",
    "# y = e ** x\n",
    "# dy/dx = e ** x = y\n",
    "dnorm_logits = counts * dcounts # chain rule\n",
    "\n",
    "# dlogit_maxes\n",
    "# norm_logits = logits - logit_maxes\n",
    "# y = a - x\n",
    "# dy/dx = -1\n",
    "# however, norm_logits.shape = [n, vocab_size], logit_maxes.shape = [n, 1]\n",
    "# so, need to do backprop for broadcasting(replication) as well, sum across rows\n",
    "dlogit_maxes = -(torch.ones_like(logit_maxes) * dnorm_logits).sum(1, keepdim=True)\n",
    "\n",
    "# dlogits\n",
    "# contribution 1\n",
    "# norm_logits = logits - logit_maxes\n",
    "# y = x - a\n",
    "# dy/dx = 1\n",
    "dlogits = torch.ones_like(logits) * dnorm_logits # this will make approximate=True but not exact\n",
    "\n",
    "# contribution 2\n",
    "# logit_maxes = logits.max(1, keepdim=True)\n",
    "# so dlogit_maxes/dlogits = 0 for non-max elements in logits and 1 for the max element\n",
    "# start with torch.zeros and then make the max elements 1\n",
    "# clean way -> (logits == logit_maxes).float() -> will be 1 for max elements and 0 otherwise\n",
    "dlogits = dlogits + ((logits == logit_maxes).float()) * dlogit_maxes\n",
    "\n",
    "# dh\n",
    "# logits = h @ W2 + b2\n",
    "# shapes: W2 = [n_hidden, vocab_size], h = [n, n_hidden], logits = [n, vocab_size]\n",
    "# dlogits/dh comes out to be upstream gradient (dlogits) @ transpose of W2\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "dh = dlogits @ W2.T\n",
    "\n",
    "#dW2\n",
    "# logits = h @ W2 + b2\n",
    "# shapes: W2 = [n_hidden, vocab_size], h = [n, n_hidden], logits = [n, vocab_size]\n",
    "# dlogits/dW2 comes out to be transpose of upstream gradient(dlogits.T) @ h\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "dW2 = (dlogits.T @ h).T\n",
    "\n",
    "#db2\n",
    "# logits = h @ W2 + b2\n",
    "# shapes: W2 = [n_hidden, vocab_size], h = [n, n_hidden], logits = [n, vocab_size]\n",
    "# dlogits/db2 comes to be sum of dlogits summed over the columns\n",
    "# it can also be written as the matmul of dlogits.T and the ones column vector of shape dlogits\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "#db2 = (dlogits.T @ torch.ones((dlogits.shape[0],1))).squeeze() # approximate = True, exact = False, possibly due to matmuls and float errors, mathematically same as below\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "# hpreact\n",
    "# h = torch.tanh(hpreact)\n",
    "# shapes: same\n",
    "# y = tanh(x), dy/dx = 1-(tanh(x) ** 2) = 1-y**2\n",
    "dhpreact = (1.0-h**2) * dh # approximate = True, exact = False\n",
    "\n",
    "# dbatch_norm_gain\n",
    "# hpreact = batch_norm_gain * bnraw + batch_norm_bias\n",
    "# shapes: hpreact: [n, n_hidden], bnraw: [n, n_hidden], batch_norm_gain: [1, n_hidden], batch_norm_bias: [1, n_hidden]\n",
    "# y = a * x + b\n",
    "# dy/dx = a\n",
    "# batch_norm_gain is broadcasted to [n, n_hidden] and then element-wise multiplied with bnraw\n",
    "# so, need to sum over rows for dbatch_norm_gain\n",
    "dbatch_norm_gain = (bnraw * dhpreact).sum(0, keepdim=True) # approximate = True, exact = False\n",
    "\n",
    "# dbatch_norm_bias\n",
    "# hpreact = batch_norm_gain * bnraw + batch_norm_bias\n",
    "# shapes: hpreact: [n, n_hidden], bnraw: [n, n_hidden], batch_norm_gain: [1, n_hidden], batch_norm_bias: [1, n_hidden]\n",
    "# y = a * x + b\n",
    "# dy/db = 1\n",
    "# batch_norm_bias is broadcasted to [n, n_hidden] and then element-wise added to bnraw * batch_norm_gain\n",
    "# so, need to sum over rows for dbatch_norm_bias\n",
    "dbatch_norm_bias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "# dbnraw\n",
    "# hpreact = batch_norm_gain * bnraw + batch_norm_bias\n",
    "# shapes: same\n",
    "# y = a * x + b\n",
    "# dy/da = x\n",
    "# no need for broadcasting as bnraw and dhpreact have same shape\n",
    "dbnraw = batch_norm_gain * dhpreact\n",
    "\n",
    "# dbndiff\n",
    "# contribution 1 --- from bnraw\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# shapes: bnraw: [n, n_hidden], bndiff: [n, n_hidden], bnvar_inv: [1, n_hidden]\n",
    "# y = x * a\n",
    "# dy/dx = a\n",
    "# no need for broadcasting as bnraw and bndiff have same shape\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "\n",
    "# dbnvar_inv\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# shapes: bnraw: [n, n_hidden], bndiff: [n, n_hidden], bnvar_inv: [1, n_hidden]\n",
    "# y = x * a\n",
    "# dy/da = x\n",
    "# bnvar_inv is broadcasted to [n, n_hidden] and then element-wise multiplied to bndiff\n",
    "# so, need to sum over rows for dbnvar_inv\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "\n",
    "# dbnvar\n",
    "# bnvar_inv = (bnvar + epsilon) ** -0.5\n",
    "# shapes: [1, n_hidden], same\n",
    "# y = (x + a)**-0.5\n",
    "# dy/dx = (-0.5(x + a)**-1.5) * (d (x + a) /dx) => -0.5 (x + a)**-1.5 => -0.5 y**3\n",
    "dbnvar = (-0.5 * bnvar_inv ** 3) * dbnvar_inv\n",
    "\n",
    "# dbndiff2\n",
    "# bnvar = (\n",
    "#    1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    "#)\n",
    "# shapes: bnvar: [1, n_hidden], bndiff2: [n, n_hidden]\n",
    "# dy/dx = 1/(n-1) * torch.ones_like(bndiff2) (since for each element in bndiff2, grad will be 1)\n",
    "dbndiff2 = ((1.0 / (n-1)) * torch.ones_like(bndiff2)) * dbnvar\n",
    "\n",
    "# dbndiff\n",
    "# contribution 2 --- from bndiff2\n",
    "# bndiff2 = bndiff**2\n",
    "# shapes: same: [n, n_hidden]\n",
    "# y = x ** 2\n",
    "# dy/dx = 2 * x + prev_grad\n",
    "dbndiff += ((2.0 * bndiff) * dbndiff2)\n",
    "\n",
    "# dhprebn\n",
    "# contribution 1 --- from bndiff\n",
    "# bndiff = hprebn - bnmeani\n",
    "# shapes: bndiff: [n, n_hidden], hprebn: [n, n_hidden], bnmeani: [1, n_hidden]\n",
    "# y = x - a\n",
    "# dy/dx = 1 (torch.ones_like(hprebn))\n",
    "# no need for broadcasting as hprebn and bndiff have same shape\n",
    "dhprebn = dbndiff.clone()\n",
    "\n",
    "# dbnmeani\n",
    "# bndiff = hprebn - bnmeani\n",
    "# shapes: bndiff: [n, n_hidden], hprebn: [n_hidden], bnmeani: [1, n_hidden]\n",
    "# y = x - a\n",
    "# dy/da = -1 -(torch.ones_like(bnmeani))\n",
    "# need to sum over dbnmeani, due to row-wise replication\n",
    "dbnmeani = (- 1.0 * (dbndiff)).sum(0, keepdim=True)\n",
    "\n",
    "# dhprebn\n",
    "# contribution 2 --- from bnmeani\n",
    "# bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "# bnmeani: [1, n_hidden], hprebn: [n, n_hidden]\n",
    "# dy/dx = 1/(n) * torch.ones_like(hprebn) + prev_grad (since for each element in hprebn, grad will be 1) + prev_grad\n",
    "dhprebn += ((1.0 / n) * torch.ones_like(hprebn)) * dbnmeani\n",
    "\n",
    "# dembcat\n",
    "# hprebn = embcat @ W1 + b1\n",
    "# shapes: hprebn: [n, n_hidden], embcat: [n, block_size * dim_size], W1: [block_size * dim_size, n_hidden], b1: [n_hidden]\n",
    "# d = a@b+c\n",
    "# dy/da = upstream_grad @ b.T\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "dembcat = dhprebn @ W1.T\n",
    "\n",
    "# dW1\n",
    "# hprebn = embcat @ W1 + b1\n",
    "# shapes: hprebn: [n, n_hidden], embcat: [n, block_size * dim_size], W1: [block_size * dim_size, n_hidden], b1: [n_hidden]\n",
    "# d = a@b+c\n",
    "# dy/db = a.T @ upstream_grad\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "dW1 = embcat.T @ dhprebn\n",
    "\n",
    "# db1\n",
    "# hprebn = embcat @ W1 + b1\n",
    "# shapes: hprebn: [n, n_hidden], embcat: [n, block_size * dim_size], W1: [block_size * dim_size, n_hidden], b1: [n_hidden]\n",
    "# d = a@b+c\n",
    "# dy/dc = upstream_grad row_wise column element sum\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "db1 = dhprebn.sum(0, keepdim=True)\n",
    "\n",
    "# demb\n",
    "# embcat = emb.view(emb.shape[0], -1)\n",
    "# shapes: embcat: [n, block_size * dim_size], emb: [n, dim_size, block_size]\n",
    "# dy/dx: just a dim change is occurring, so backprop would be to flatten the view back to original dimensions.\n",
    "demb = dembcat.view_as(emb)\n",
    "\n",
    "#dC\n",
    "# emb = C[Xb]\n",
    "# shapes: emb: [n, block_size, dim_size], C: [vocab_size, dim_size], Xb: [n, block_size]\n",
    "# In forward pass, what's happening for each 3-dim example in Xb (the integer encoding of the character blocks), the corresponding dimension (from C) is fetched and organized into emb.\n",
    "# So to generate dC, we need to get the vocab's corresponding gradient that's sitting in demb and add it to dc\n",
    "# But we need to accumulate gradients as one vocab letter may be used in multiple examples in Xb, and therefore emb.\n",
    "# so dC will be a tensor of zeros of shape like C, but at the indices of Xb, the value will be 1\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k, j]\n",
    "            dC[ix] += demb[k, j]\n",
    "\n",
    "\n",
    "# All comparisons\n",
    "cmp('logprob', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('dcounts_sum', dcounts_sum, counts_sum)\n",
    "cmp('dcounts', dcounts, counts)\n",
    "cmp('dnorm_logits', dnorm_logits, norm_logits)\n",
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('dlogits', dlogits, logits)\n",
    "cmp('dh', dh, h)\n",
    "cmp('dW2', dW2, W2)\n",
    "cmp('db2', db2, b2)\n",
    "cmp('dhpreact', dhpreact, hpreact)\n",
    "cmp('dbatch_norm_gain', dbatch_norm_gain, batch_norm_gain)\n",
    "cmp('dbatch_norm_bias', dbatch_norm_bias, batch_norm_bias)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('dbnvar', dbnvar, bnvar)\n",
    "cmp('dbndiff2', dbndiff2, bndiff2)\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "cmp('demb', demb, emb)\n",
    "cmp('dC', dC, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"#appendix\">Go to Appendix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<a href=\"#appendix\">Go to Appendix</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_fast: 3.423457622528076 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# backward pass for cross-entropy in one go\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(f\"Loss_fast: {loss_fast} diff: {loss_fast.item() - loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact: False | approximate: True  | maxdiff: 5.820766091346741e-09\n"
     ]
    }
   ],
   "source": [
    "# Before: forward pass for cross-entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes  # subtract max for numerical stability ??\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdim=True)\n",
    "# counts_sum_inv = counts_sum**-1\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# for a single logit example, the derivative of the loss w.r.t the logit example comes out to be the following:\n",
    "# --- softmax of the logit (when the logit's index is not equal to Yb)\n",
    "# --- softmax of the logit - 1 (when the logit's index is equal to Yb)\n",
    "# see appendix for handwritten impl.\n",
    "# need to also scale down the derivative over the average, since the final loss is the average over the losses of all the examples.\n",
    "dlogits = F.softmax(logits, dim=1) # need to do softmax over the rows\n",
    "dlogits[range(logits.shape[0]), Yb] -= 1.0\n",
    "dlogits /= n\n",
    "cmp('dlogits', dlogits, logits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0353, 0.0235, 0.0194, 0.0236, 0.0258, 0.0723, 0.0369, 0.0350, 0.0318,\n",
       "        0.0250, 0.0224, 0.0345, 0.0686, 0.0481, 0.0216, 0.0240, 0.0278, 0.0236,\n",
       "        0.0346, 0.0547, 0.0469, 0.0159, 0.0377, 0.1067, 0.0634, 0.0182, 0.0227],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0353,  0.0235,  0.0194,  0.0236,  0.0258,  0.0723,  0.0369,  0.0350,\n",
       "         0.0318,  0.0250,  0.0224,  0.0345,  0.0686,  0.0481,  0.0216,  0.0240,\n",
       "        -0.9722,  0.0236,  0.0346,  0.0547,  0.0469,  0.0159,  0.0377,  0.1067,\n",
       "         0.0634,  0.0182,  0.0227], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x73b131f674d0>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: 2.09808349609375e-05\n"
     ]
    }
   ],
   "source": [
    "# backprop through batchnorm in one go\n",
    "# IMP: Look at the expression for the output of batchnorm, \n",
    "# take the derivative w.r.t the input, simplify the expression and just write it out\n",
    "\n",
    "# forward pass\n",
    "# Before:\n",
    "# bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = (\n",
    "#     1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    "# )  # Bessel's correction, dividing by (n-1) not n, for mean calc\n",
    "# with torch.no_grad():\n",
    "#     epsilon = 1e-5\n",
    "# bnvar_inv = (bnvar + epsilon) ** -0.5  # inverse of \"bnstd\"\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = batch_norm_gain * bnraw + batch_norm_bias\n",
    "\n",
    "# now\n",
    "hpreact_fast = batch_norm_gain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True)) + batch_norm_bias\n",
    "print(f\"diff: {(hpreact_fast - hpreact).abs().max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Appendix"
    ]
   },
   "source": [
    "# Appendix\n",
    "## Matmul Backprop Calculation\n",
    "![Matmul Backprop Steps](assets/matmul_backprop/IMG_20250131_191121501_HDR.jpg)\n",
    "### (Continued)\n",
    "![Matmul Backprop Steps (contd.)](assets/matmul_backprop/IMG_20250131_191127782_HDR.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
