{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional, Any\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "with open('names.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "vocab = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i for i,s in enumerate(vocab, start=1)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test split\n",
    "random.shuffle(words)\n",
    "\n",
    "train_split = .8; val_split = .1\n",
    "\n",
    "n1 = int(train_split * len(words))\n",
    "n2 = int((train_split + val_split) * len(words))\n",
    "\n",
    "X_train, Y_train = build_dataset(words[:n1])\n",
    "X_val, Y_val = build_dataset(words[n1:n2])\n",
    "X_test, Y_test = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility fn to compare gradients calc manually v/s via PyTorch\n",
    "def cmp(s, dt: torch.Tensor, t: torch.Tensor):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f\"{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "#IMP FOR THIS NOTEBOOK: CHANGE SOME INIT VALUES AS ZEROS MAY MASK SOME INCORRECT GRADIENT IMPL, **JUST FOR LEARNING PURPOSES**\n",
    "\n",
    "# build MLP\n",
    "\n",
    "# MLP structure params\n",
    "emb_dim_size = 10\n",
    "hidden_layer_size = 64\n",
    "\n",
    "# MLP params\n",
    "C = torch.randn((vocab_size, emb_dim_size), generator=g).float() # represent each character in dim space\n",
    "#Layer 1\n",
    "W1 = torch.randn((block_size * emb_dim_size, hidden_layer_size), generator=g).float() # weights for hidden layer, for each neuron, it will receive block_size number of i/p, each i/p of dimension dim_size\n",
    "W1 = W1 * (5/3) / ((block_size * emb_dim_size) ** 0.5) # kiming_init - to \"fight the contraction of tanh\", preserve gaussian std. -> to resolve tanh saturation\n",
    "b1 = torch.randn(hidden_layer_size, generator=g).float() # each neuron will have 1-D bias # USING BIAS JUST FOR FUN\n",
    "b1 = b1 * 0.1 # normally: 0.001 # to resolve tanh saturation\n",
    "# Layer 2\n",
    "W2 = torch.randn((hidden_layer_size, vocab_size), generator=g).float() # weights for output layer, each neuron of hidden layer fully connected to each neuron of output layer\n",
    "W2 = W2 * 0.1 # to normalize loss at initialization\n",
    "b2 = torch.randn(vocab_size, generator=g).float() # num_neurons in output layer equal to vocab size, to represent probs for each character\n",
    "b2 = b2 * 0.1 # normally 0 # to normalize loss at initialization\n",
    "\n",
    "## batch norm params: want the distribution to be more flexible, not constrict it to be always Gaussian, just need at initialization.\n",
    "# batch_norm_gain = torch.ones((1, hidden_layer_size))\n",
    "# batch_norm_bias = torch.zeros((1, hidden_layer_size))\n",
    "# change from normal, normal above\n",
    "batch_norm_gain = torch.randn((1, hidden_layer_size)) * 0.1 + 1.0\n",
    "batch_norm_bias = torch.randn((1, hidden_layer_size)) * 0.1\n",
    "\n",
    "params = [C, W1, b1, W2, b2, batch_norm_gain, batch_norm_bias]\n",
    "\n",
    "running_mean = torch.zeros((1, hidden_layer_size))\n",
    "running_std = torch.ones((1, hidden_layer_size))\n",
    "\n",
    "print(sum(p.nelement() for p in params))\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "n = batch_size\n",
    "# construct minibatch\n",
    "ix = torch.randint(0, X_train.shape[0], (n, ), generator=g)\n",
    "Xb, Yb = X_train[ix], Y_train[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5347, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FORWARD PASS: CHUNKED INTO SIMPLE, MANAGEABLE STEPS THAT CAN BE BACKPROP'ED ONE AT A TIME\n",
    "\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "# -----------Linear layer 1---------------\n",
    "hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "# ----------BatchNorm layer---------------\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = (\n",
    "    1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    ")  # Bessel's correction, dividing by (n-1) not n, for mean calc\n",
    "with torch.no_grad():\n",
    "    epsilon = 1e-5\n",
    "bnvar_inv = (bnvar + epsilon) ** -0.5  # inverse of \"bnstd\"\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = batch_norm_gain * bnraw + batch_norm_bias\n",
    "# ----------Non-linearity---------------\n",
    "h = torch.tanh(hpreact)\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "# cross-entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability ??\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# Pytorch backward pass\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "\n",
    "\n",
    "gradvars = [\n",
    "    logprobs,\n",
    "    probs,\n",
    "    counts,\n",
    "    counts_sum,\n",
    "    counts_sum_inv,\n",
    "    norm_logits,\n",
    "    logit_maxes,\n",
    "    logits,\n",
    "    h,\n",
    "    hpreact,\n",
    "    bnraw,\n",
    "    bnvar_inv,\n",
    "    bnvar,\n",
    "    bndiff2,\n",
    "    bndiff,\n",
    "    hprebn,\n",
    "    bnmeani,\n",
    "    embcat,\n",
    "    emb\n",
    "]\n",
    "\n",
    "for t in gradvars:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhpreact        | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dbatch_norm_gain | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dbatch_norm_bias | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "dbnraw          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dbnvar_inv      | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dbnvar          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dbndiff2        | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
      "dbndiff         | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
      "dbnmeani        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dhprebn         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dembcat         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dW1             | exact: False | approximate: True  | maxdiff: 6.28642737865448e-09\n",
      "db1             | exact: False | approximate: True  | maxdiff: 5.238689482212067e-09\n",
      "demb            | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n"
     ]
    }
   ],
   "source": [
    "#backprop through the grads array manually\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n # loss = -dlogprobs[range(n), Yb].mean() => loss = -(a + b + c) / 3 => dloss/da = -1/3 n = 32 in our case (batch size)\n",
    "\n",
    "dprobs = 1.0/probs # logprobs = probs.log() => log(a) => dlogprobs/da = 1/a\n",
    "dprobs = dprobs * dlogprobs # chain rule\n",
    "\n",
    "# c = a * b\n",
    "# tensor\n",
    "# c[3x3] = a[3x3] * b[3x1]\n",
    "# c1 = a11*b1  a12*b1  a13*b1\n",
    "# c2 = a21*b2  a22*b2  a23*b2\n",
    "# c3 = a31*b1  a32*b2  a33*b3\n",
    "\n",
    "dcounts_sum_inv = counts # probs = counts * counts_sum_inv => probs = b * a, dprobs/da = b\n",
    "dcounts_sum_inv = dcounts_sum_inv * dprobs # chain rule\n",
    "dcounts_sum_inv = dcounts_sum_inv.sum(1, keepdim=True) # sum through pytorch replication => m1 [3X1] => m2 [3x3] so gradients would be summed across rows\n",
    "# dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "\n",
    "# dcounts --- 1st contribution (from probs)\n",
    "dcounts = counts_sum_inv # probs = counts * counts_sum_inv => probs = b * a, dprobs/db = a\n",
    "dcounts = dcounts * dprobs # chain rule #TODO: Investigate why dcounts *= dprobs doesn't work here. Look at shapes for hint.\n",
    "\n",
    "dcounts_sum = (-counts_sum ** -2)# counts_sum_inv = counts_sum ** -1, y = x**-1, dy/dx = -x**-2\n",
    "dcounts_sum = dcounts_sum * dcounts_sum_inv # chain rule\n",
    "\n",
    "# dcounts --- 2nd contribution (from counts_sum)\n",
    "# a[3x3] -> b[3x1] (rows summed)\n",
    "# a11 a12 a13   b1 = a11 + a12 + a13\n",
    "# a21 a22 a23   b2 = a21 + a22 + a23\n",
    "# a31 a32 a33   b3 = a31 + a32 + a33\n",
    "\n",
    "# db/da = [1 1 1] (3x1)\n",
    "\n",
    "dcounts = dcounts + (torch.ones(counts.shape) * dcounts_sum)\n",
    "\n",
    "# dnorm_logits\n",
    "# counts = norm_logits.exp() # both of same shape\n",
    "# y = e ** x\n",
    "# dy/dx = e ** x = y\n",
    "dnorm_logits = counts * dcounts # chain rule\n",
    "\n",
    "# dlogit_maxes\n",
    "# norm_logits = logits - logit_maxes\n",
    "# y = a - x\n",
    "# dy/dx = -1\n",
    "# however, norm_logits.shape = [n, vocab_size], logit_maxes.shape = [n, 1]\n",
    "# so, need to do backprop for broadcasting(replication) as well, sum across rows\n",
    "dlogit_maxes = -(torch.ones_like(logit_maxes) * dnorm_logits).sum(1, keepdim=True)\n",
    "\n",
    "# dlogits\n",
    "# contribution 1\n",
    "# norm_logits = logits - logit_maxes\n",
    "# y = x - a\n",
    "# dy/dx = 1\n",
    "dlogits = torch.ones_like(logits) * dnorm_logits # this will make approximate=True but not exact\n",
    "\n",
    "# contribution 2\n",
    "# logit_maxes = logits.max(1, keepdim=True)\n",
    "# so dlogit_maxes/dlogits = 0 for non-max elements in logits and 1 for the max element\n",
    "# start with torch.zeros and then make the max elements 1\n",
    "# clean way -> (logits == logit_maxes).float() -> will be 1 for max elements and 0 otherwise\n",
    "dlogits = dlogits + ((logits == logit_maxes).float()) * dlogit_maxes\n",
    "\n",
    "# dh\n",
    "# logits = h @ W2 + b2\n",
    "# shapes: W2 = [n_hidden, vocab_size], h = [n, n_hidden], logits = [n, vocab_size]\n",
    "# dlogits/dh comes out to be upstream gradient (dlogits) @ transpose of W2\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "dh = dlogits @ W2.T\n",
    "\n",
    "#dW2\n",
    "# logits = h @ W2 + b2\n",
    "# shapes: W2 = [n_hidden, vocab_size], h = [n, n_hidden], logits = [n, vocab_size]\n",
    "# dlogits/dW2 comes out to be transpose of upstream gradient(dlogits.T) @ h\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "dW2 = (dlogits.T @ h).T\n",
    "\n",
    "#db2\n",
    "# logits = h @ W2 + b2\n",
    "# shapes: W2 = [n_hidden, vocab_size], h = [n, n_hidden], logits = [n, vocab_size]\n",
    "# dlogits/db2 comes to be sum of dlogits summed over the columns\n",
    "# it can also be written as the matmul of dlogits.T and the ones column vector of shape dlogits\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "#db2 = (dlogits.T @ torch.ones((dlogits.shape[0],1))).squeeze() # approximate = True, exact = False, possibly due to matmuls and float errors, mathematically same as below\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "# hpreact\n",
    "# h = torch.tanh(hpreact)\n",
    "# shapes: same\n",
    "# y = tanh(x), dy/dx = 1-(tanh(x) ** 2) = 1-y**2\n",
    "dhpreact = (1.0-h**2) * dh # approximate = True, exact = False\n",
    "\n",
    "# dbatch_norm_gain\n",
    "# hpreact = batch_norm_gain * bnraw + batch_norm_bias\n",
    "# shapes: hpreact: [n, n_hidden], bnraw: [n, n_hidden], batch_norm_gain: [1, n_hidden], batch_norm_bias: [1, n_hidden]\n",
    "# y = a * x + b\n",
    "# dy/dx = a\n",
    "# batch_norm_gain is broadcasted to [n, n_hidden] and then element-wise multiplied with bnraw\n",
    "# so, need to sum over rows for dbatch_norm_gain\n",
    "dbatch_norm_gain = (bnraw * dhpreact).sum(0, keepdim=True) # approximate = True, exact = False\n",
    "\n",
    "# dbatch_norm_bias\n",
    "# hpreact = batch_norm_gain * bnraw + batch_norm_bias\n",
    "# shapes: hpreact: [n, n_hidden], bnraw: [n, n_hidden], batch_norm_gain: [1, n_hidden], batch_norm_bias: [1, n_hidden]\n",
    "# y = a * x + b\n",
    "# dy/db = 1\n",
    "# batch_norm_bias is broadcasted to [n, n_hidden] and then element-wise added to bnraw * batch_norm_gain\n",
    "# so, need to sum over rows for dbatch_norm_bias\n",
    "dbatch_norm_bias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "# dbnraw\n",
    "# hpreact = batch_norm_gain * bnraw + batch_norm_bias\n",
    "# shapes: same\n",
    "# y = a * x + b\n",
    "# dy/da = x\n",
    "# no need for broadcasting as bnraw and dhpreact have same shape\n",
    "dbnraw = batch_norm_gain * dhpreact\n",
    "\n",
    "# dbndiff\n",
    "# contribution 1 --- from bnraw\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# shapes: bnraw: [n, n_hidden], bndiff: [n, n_hidden], bnvar_inv: [1, n_hidden]\n",
    "# y = x * a\n",
    "# dy/dx = a\n",
    "# no need for broadcasting as bnraw and bndiff have same shape\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "\n",
    "# dbnvar_inv\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# shapes: bnraw: [n, n_hidden], bndiff: [n, n_hidden], bnvar_inv: [1, n_hidden]\n",
    "# y = x * a\n",
    "# dy/da = x\n",
    "# bnvar_inv is broadcasted to [n, n_hidden] and then element-wise multiplied to bndiff\n",
    "# so, need to sum over rows for dbnvar_inv\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "\n",
    "# dbnvar\n",
    "# bnvar_inv = (bnvar + epsilon) ** -0.5\n",
    "# shapes: [1, n_hidden], same\n",
    "# y = (x + a)**-0.5\n",
    "# dy/dx = (-0.5(x + a)**-1.5) * (d (x + a) /dx) => -0.5 (x + a)**-1.5 => -0.5 y**3\n",
    "dbnvar = (-0.5 * bnvar_inv ** 3) * dbnvar_inv\n",
    "\n",
    "# dbndiff2\n",
    "# bnvar = (\n",
    "#    1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    "#)\n",
    "# shapes: bnvar: [1, n_hidden], bndiff2: [n, n_hidden]\n",
    "# dy/dx = 1/(n-1) * torch.ones_like(bndiff2) (since for each element in bndiff2, grad will be 1)\n",
    "dbndiff2 = ((1.0 / (n-1)) * torch.ones_like(bndiff2)) * dbnvar\n",
    "\n",
    "# dbndiff\n",
    "# contribution 2 --- from bndiff2\n",
    "# bndiff2 = bndiff**2\n",
    "# shapes: same: [n, n_hidden]\n",
    "# y = x ** 2\n",
    "# dy/dx = 2 * x + prev_grad\n",
    "dbndiff += ((2.0 * bndiff) * dbndiff2)\n",
    "\n",
    "# dhprebn\n",
    "# contribution 1 --- from bndiff\n",
    "# bndiff = hprebn - bnmeani\n",
    "# shapes: bndiff: [n, n_hidden], hprebn: [n, n_hidden], bnmeani: [1, n_hidden]\n",
    "# y = x - a\n",
    "# dy/dx = 1 (torch.ones_like(hprebn))\n",
    "# no need for broadcasting as hprebn and bndiff have same shape\n",
    "dhprebn = dbndiff.clone()\n",
    "\n",
    "# dbnmeani\n",
    "# bndiff = hprebn - bnmeani\n",
    "# shapes: bndiff: [n, n_hidden], hprebn: [n_hidden], bnmeani: [1, n_hidden]\n",
    "# y = x - a\n",
    "# dy/da = -1 -(torch.ones_like(bnmeani))\n",
    "# need to sum over dbnmeani, due to row-wise replication\n",
    "dbnmeani = (- 1.0 * (dbndiff)).sum(0, keepdim=True)\n",
    "\n",
    "# dhprebn\n",
    "# contribution 2 --- from bnmeani\n",
    "# bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "# bnmeani: [1, n_hidden], hprebn: [n, n_hidden]\n",
    "# dy/dx = 1/(n) * torch.ones_like(hprebn) + prev_grad (since for each element in hprebn, grad will be 1) + prev_grad\n",
    "dhprebn += ((1.0 / n) * torch.ones_like(hprebn)) * dbnmeani\n",
    "\n",
    "# dembcat\n",
    "# hprebn = embcat @ W1 + b1\n",
    "# shapes: hprebn: [n, n_hidden], embcat: [n, block_size * dim_size], W1: [block_size * dim_size, n_hidden], b1: [n_hidden]\n",
    "# d = a@b+c\n",
    "# dy/da = upstream_grad @ b.T\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "dembcat = dhprebn @ W1.T\n",
    "\n",
    "# dW1\n",
    "# hprebn = embcat @ W1 + b1\n",
    "# shapes: hprebn: [n, n_hidden], embcat: [n, block_size * dim_size], W1: [block_size * dim_size, n_hidden], b1: [n_hidden]\n",
    "# d = a@b+c\n",
    "# dy/db = a.T @ upstream_grad\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "dW1 = embcat.T @ dhprebn\n",
    "\n",
    "# db1\n",
    "# hprebn = embcat @ W1 + b1\n",
    "# shapes: hprebn: [n, n_hidden], embcat: [n, block_size * dim_size], W1: [block_size * dim_size, n_hidden], b1: [n_hidden]\n",
    "# d = a@b+c\n",
    "# dy/dc = upstream_grad row_wise column element sum\n",
    "# check appendix complete handwritten impl. (navigate using below cell)\n",
    "db1 = dhprebn.sum(0, keepdim=True)\n",
    "\n",
    "# demb\n",
    "# embcat = emb.view(emb.shape[0], -1)\n",
    "# shapes: embcat: [n, block_size * dim_size], emb: [n, dim_size, block_size]\n",
    "# dy/dx: just a dim change is occurring, so backprop would be to flatten the view back to original dimensions.\n",
    "demb = dembcat.view_as(emb)\n",
    "\n",
    "# All comparisons\n",
    "cmp('logprob', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('dcounts_sum', dcounts_sum, counts_sum)\n",
    "cmp('dcounts', dcounts, counts)\n",
    "cmp('dnorm_logits', dnorm_logits, norm_logits)\n",
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('dlogits', dlogits, logits)\n",
    "cmp('dh', dh, h)\n",
    "cmp('dW2', dW2, W2)\n",
    "cmp('db2', db2, b2)\n",
    "cmp('dhpreact', dhpreact, hpreact)\n",
    "cmp('dbatch_norm_gain', dbatch_norm_gain, batch_norm_gain)\n",
    "cmp('dbatch_norm_bias', dbatch_norm_bias, batch_norm_bias)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('dbnvar', dbnvar, bnvar)\n",
    "cmp('dbndiff2', dbndiff2, bndiff2)\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "cmp('demb', demb, emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"#appendix\">Go to Appendix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<a href=\"#appendix\">Go to Appendix</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Appendix"
    ]
   },
   "source": [
    "# Appendix\n",
    "## Matmul Backprop Calculation\n",
    "![Matmul Backprop Steps](assets/matmul_backprop/IMG_20250131_191121501_HDR.jpg)\n",
    "### (Continued)\n",
    "![Matmul Backprop Steps (contd.)](assets/matmul_backprop/IMG_20250131_191127782_HDR.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
